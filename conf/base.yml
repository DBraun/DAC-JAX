# Model setup
DAC.sample_rate: 44100
DAC.encoder_dim: 64
DAC.encoder_rates: [2, 4, 8, 8]
DAC.decoder_dim: 1536
DAC.decoder_rates: [8, 8, 4, 2]

# Quantization
DAC.n_codebooks: 9
DAC.codebook_size: 1024
DAC.codebook_dim: 8
DAC.quantizer_dropout: 1.0

# Discriminator
Discriminator.sample_rate: 44100
Discriminator.rates: []
Discriminator.periods: [2, 3, 5, 7, 11]
Discriminator.fft_sizes: [2048, 1024, 512]
Discriminator.bands:
  - [0.0, 0.1]
  - [0.1, 0.25]
  - [0.25, 0.5]
  - [0.5, 0.75]
  - [0.75, 1.0]

# Schedules
create_generator_schedule.learning_rate: 1e-4
create_generator_schedule.lr_gamma: 0.999996

create_discriminator_schedule.learning_rate: 1e-4
create_discriminator_schedule.lr_gamma: 0.999996

# Optimization
create_generator_optimizer.adam_b1: 0.8
create_generator_optimizer.adam_b2: 0.99
create_generator_optimizer.weight_decay: .01
create_generator_optimizer.grad_clip: 1e3

create_discriminator_optimizer.adam_b1: 0.8
create_discriminator_optimizer.adam_b2: 0.99
create_discriminator_optimizer.weight_decay: .01
create_discriminator_optimizer.grad_clip: 10

lambdas:
  mel/loss: 15.0
  adv/feat_loss: 200  # 2.0 * (5+5+5+5+5+25+25+25) = 2.0 * 100
                      # 2.0 comes from the PyTorch DAC base.yml Then we multiply since we normalized the magnitude
                      # of our feature loss differently than the PyTorch version.
                      # 5 is (6-1) where 6 is number of convs in MPD. Then there are 5 of these because the
                      # number of periods is 5.
                      # 25 is number of bands (5) times the number of convs (5) in MRD. Then there are 3 of these
                      # because of the number of fft sizes is 3.
  adv/gen_loss: 8  # 1.0 * 8 where 8 is number of Discriminator rates+periods+fft sizes = (0+5+3)
                   # 1.0 comes from the PyTorch DAC base.yml
  vq/commitment_loss: 2.25  # 0.25 * 9 since we normalize based on the number of codebooks.
  vq/codebook_loss: 9  # 1 * 9 since we normalized based on the number of codebooks.

train.batch_size: 72
train.val_batch_size: 100
train.sample_batch_size: 100
train.num_iterations: 250000
train.valid_freq: 1000
train.sample_freq: 10000
train.ckpt_max_keep: 4
train.seed: 0
train.tabulate: 1

EarlyStopping.min_delta: .001
EarlyStopping.patience: 4

log_training.log_every_steps: 10

# Loss setup
multiscale_stft_loss.window_lengths: [2048, 512]

mel_spectrogram_loss.n_mels: [5, 10, 20, 40, 80, 160, 320]
mel_spectrogram_loss.window_lengths: [32, 64, 128, 256, 512, 1024, 2048]
mel_spectrogram_loss.lower_edge_hz: [0, 0, 0, 0, 0, 0, 0]
mel_spectrogram_loss.upper_edge_hz: [null, null, null, null, null, null, null]
mel_spectrogram_loss.pow: 1.0
mel_spectrogram_loss.clamp_eps: 1.0e-5
mel_spectrogram_loss.mag_weight: 0.0

# Data Augmentation
VolumeNorm.config:
  min_db: -16
  max_db: -16

train/build_transforms.augment:
  - VolumeNorm
  - RescaleAudio
  - ShiftPhase

val/build_transforms.augment:
  - VolumeNorm
  - RescaleAudio

sample/build_transforms.augment:
  - VolumeNorm
  - RescaleAudio

# Data
# See salient_excerpt in Descript AudioTools.
train/SaliencyParams.enabled: 1
train/SaliencyParams.num_tries: 8
train/SaliencyParams.loudness_cutoff: -40

# Data
create_dataset.extensions:
  - .wav
  - .flac
  - .ogg
  - .mp3

train/create_dataset.duration: 0.38
val/create_dataset.duration: 5.0
sample/create_dataset.duration: 5.0
test/create_dataset.duration: 10.0

val/create_dataset.num_steps: 4

train/create_dataset.sources:
  speech_fb:
    - /data/daps/train
  speech_hq:
    - /data/vctk
    - /data/vocalset
    - /data/read_speech
    - /data/french_speech
  speech_uq:
    - /data/emotional_speech/
    - /data/common_voice/
    - /data/german_speech/
    - /data/russian_speech/
    - /data/spanish_speech/
  music_hq:
    - /data/musdb/train
  music_uq:
    - /data/jamendo
  general:
    - /data/audioset/data/unbalanced_train_segments/
    - /data/audioset/data/balanced_train_segments/

val/create_dataset.sources:
  speech_hq:
    - /data/daps/val
  music_hq:
    - /data/musdb/test
  general:
    - /data/audioset/data/eval_segments/

sample/create_dataset.sources:
  speech_hq:
    - /data/daps/val
  music_hq:
    - /data/musdb/test
  general:
    - /data/audioset/data/eval_segments/

test/create_dataset.sources:
  speech_hq:
    - /data/daps/test
  music_hq:
    - /data/musdb/test
  general:
    - /data/audioset/data/eval_segments/
